{
	"nodes":[
		{"id":"d12fbfef389c0fe0","type":"text","text":"**Sources of Error in Prediction**\nWe cannot hope to fit a function perfectly to y, since y = f(x) + e contains noise e. \n*Irreducible Error* - Variability associated with noise *e* which affects the accuracy of our predictions. This noise may include unmeasured variables that may be useful in predicting *y*, but since we do not measure them, *f* cannot use them for its prediction. \n*Reducible Error* - Reducible because we can potentially improve the accuracy of *ð‘“^* by using the most appropriate learning technique to estimate *f*. \n![[Pasted image 20251002182922.png]]","x":-460,"y":-500,"width":517,"height":460},
		{"id":"60d237075d3f0e20","type":"file","file":"Assets/Pasted image 20251002183330.png","x":-1458,"y":-500,"width":998,"height":336},
		{"id":"69f3da7a87dfcb63","type":"text","text":"**Underfitting and Overfitting**\n![[Pasted image 20251002184303.png]]\nLinear model represented with a straight line represents high bias. It *underfits the data*. \nThe model with low bias and high variance sees new data points distorting the model. By adjusting itself directly to the data, it *overfits* the training data. \nWe want a good fit, but we do not want to overfit because it means that the model will have a hard time predicting on new data or generalizing. \n","x":-1458,"y":-164,"width":998,"height":604},
		{"id":"4b5e23b8013c98d3","type":"text","text":"**Supervised Learning**","x":-398,"y":-640,"width":197,"height":59},
		{"id":"f26ffba37e185ccc","type":"text","text":"![[Pasted image 20251002175401.png]]","x":200,"y":19,"width":327,"height":114},
		{"id":"661044f96bd4da4e","type":"text","text":"1. Collect training data  \n2. Use a learning algorithm to fit a model  \n3. Use model to make a prediction","x":200,"y":133,"width":219,"height":201},
		{"id":"5d89c7cd95f36475","type":"text","text":"![[Pasted image 20251002175341.png]]","x":189,"y":-226,"width":461,"height":245},
		{"id":"80fca7f5d1e8646c","type":"text","text":"Generally, more flexible methods result in less bias and more variance. Good test of performance requires low variance as well as low squared bias. \n![[Pasted image 20251002185900.png]]\nThis is referred to as a trade-off because it is easy to obtain a method with  \nextremely low bias but high variance (for instance, by drawing a curve that  \npasses through every  \nsingle training observation) or a method with very low variance but high bias  \n(by fitting a horizontal line to the data).","x":-460,"y":-40,"width":517,"height":420},
		{"id":"af454bf0967cabcc","type":"file","file":"Assets/Pasted image 20251002190020.png","x":-460,"y":380,"width":399,"height":141},
		{"id":"5ba2ce166156ddbd","type":"text","text":"Although we have focused the analysis on regression, this tradeoff applies to all forms of supervised learning.\nIn a brief overview:\n- **bias**: errors from wrong assumptions in the model. High bias leads to a straight linear, single degree function failing to capture all of the data points or patterns in the data. \n- **variance**: Error from the model being too sensitive to the data. This is like trying to fit an overly complex high degree polynomial curve to every data point, essentially memorizing the data set. \nThe trade-off is that as model complexity increases:\n\tBias tends to go down (model fits training data better).\n\tVariance tends to go up (model overreacts to training data noise).\n**Occam's Razor**: \"Among competing explanations, the simplest one that works is usually best.\"\n- Too simple --> underfit \n- Too complex --> overfit \nSolution: Choose the simplest model that explains the data well. ","x":-1458,"y":440,"width":998,"height":360},
		{"id":"82fded892e3bbbf7","type":"text","text":"**Linear Regression**","x":223,"y":492,"width":177,"height":50},
		{"id":"745db900f02de54d","type":"text","text":"We have the output prediction given by: $$Wx+ b_{1} = \\hat{y}$$\nOur cost function is presented by: $$J(w,b) = \\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{i}-y^i)^2$$\nNotice that this is a dot product that can be written in the following way: \n$$\\frac{1}{2m}(\\hat{y}-y)^T(\\hat{y}-y)$$\n$$J(w,b) = \\frac{1}{2m}(Xw+b-y)^T(Xw+b-y)$$\nIn order to discover the minimum of our cost function, need to find the point where: $$\\frac{ \\partial J }{ \\partial w } = 0 $$","x":223,"y":1078,"width":796,"height":482},
		{"id":"a01e6de1912c8d89","type":"text","text":"How do we determine the equation that defines the coefficients for the weights?\n$$j = \\sum^{m}_{i=0}w_{j}x_{j}$$\n$$\\begin{bmatrix}\nx_{1}^1 \\dots x_{m}^1 \\\\ \\\\\n\\dots \\\\\nx_{n}^1 \\dots x_{m}^n\n\\end{bmatrix} \\begin{bmatrix}\nw_{0} \\\\\nw_{1} \\\\\n\\dots \\\\\nw_{m}\n\\end{bmatrix}$$\n$$\\hat{y} = Xw$$\n$$J(w) = \\frac{1}{2m} (\\hat{y}-y)^T(\\hat{y} - y)$$\n$$= \\frac{1}{2m}(Xw-y)^T(Xw-y)$$\nCompute the derivative wrt. *w* of *J*:\nAssume that we have a variable:\n$$\\mu = (Xw -y)$$\nUsing this variable:\n$$\\frac{d(J)}{dw} = \\frac{d(\\mu^T\\mu)}{dw} = 0$$\nFrom this step forwards, this is just the chain rule. We obtain through derivation: \n$$\\frac{dJ}{dw} = \\frac{d\\mu^T\\mu}{dw} = 2(\\frac{d\\mu}{dw})^T \\bullet \\mu$$\nWhich has to be equal to 0 in order to minimize it.\nRecall: $$\\mu = Xw-y$$\n$$ = 2 [\\frac{d}{dw}(Xw-y)]^T \\bullet (Xw - y) = 0$$\n$$X^T \\bullet (Xw-y)=0$$\n$$X^TXw - X^Ty = 0$$\n$$X^TXw = X^Ty$$\nFinally, finding the equation for w: $$w = (X^TX)^{-1}X^Ty$$\nThis equation represents the analytical **closed form** solution, but it may not solve the problem accurately if the matrix is very large. ","x":223,"y":1560,"width":796,"height":1020},
		{"id":"7df3ca732d48a3ac","type":"text","text":"```Python\ndef calc_normal_eq(x, y):\n\tn, m = np.shape(X)\n\tXTX = np.dot(X.T, X)\n\tXTX_1 = np.linalg.inv(XTX)\n\tXTy = X.T @ y\n\treturn XTX_1 @ XTy\n```","x":223,"y":2580,"width":796,"height":200},
		{"id":"92bffdefe6b29f05","type":"text","text":"**Gradient Descent**\nWe need some numerical approach that allows us to approach the minimum point over the course of some iterations. \n$$w_{new} = w_{old} + \\triangle w$$\n$$w_{new} = w_{old} - \\eta\\nabla J$$\nThe term *eta* is the learning rate hyperparameter, which can allow us to control the rate at which we increase or decrease w. \n\n![[Pasted image 20250908181222.png]]\n![[Pasted image 20250908181230.png]]\n![[Pasted image 20251002194151.png]]\n\nExtrapolate this out into 3D:\nThe graph of a multivariate function defines a surface in Euclidean space. To every point P on this surface, there are an infinite number of tangent lines.\nPartial differentiation is the act of choosing one of these lines and finding its slope. \nThe gradient is the vector that assembles the partial derivatives and represents the direction of the rate of fastest increase. \n![[Pasted image 20251002191953.png]]\n\nTo do gradient descent, we initialize the weights to some value (e.g. all zeros), and repeatedly  \nadjust them in the direction that most decreases the cost function.\nâ€¢ If we visualize the cost function as a surface, so that lower is better, this is the direction of  \nsteepest descent.  \nâ€¢ We repeat this procedure until the iterates converge, or stop changing much.  \nâ€¢ (Or, in practice, we often run it until we get tired of waiting.)  \nâ€¢ If weâ€™re lucky, the final iterate will be close to the optimum.\nSince the result of the gradient gives us the direction of maximum ascent and we want to minimize our cost function, we will choose the direction of -gradient. \n","x":1019,"y":580,"width":781,"height":2320},
		{"id":"cfc3c059360b16c8","type":"text","text":"Vectorized Gradient Descent Rule![[Pasted image 20251002195443.png]]","x":256,"y":2780,"width":731,"height":468},
		{"id":"b2fdae3b2810a026","type":"text","text":"**How do you minimize the function J without knowing much about it?**\n*Taylor Approximation*:\n![[Pasted image 20251002195804.png]]\n\nIn gradient descent, we only use the first order gradient. We can assume that the function J(w) around w is linear and behaves like J(w) = gradient(J(w))^T * delta(w)\n\n![[Pasted image 20251002195926.png]]\n\nGradient descent is basically **using the first-order Taylor approximation** of the cost function to decide how to move toward a minimum.\n\n- Taylor approximation gives the local slope.\n    \n- Gradient descent says: \"take a small step downhill along that slope.\"","x":1800,"y":580,"width":680,"height":840},
		{"id":"035684a0486a80c1","type":"text","text":"Some measures of predictive accuracy for linear regression: \n![[Pasted image 20251002200224.png]]","x":1800,"y":1420,"width":900,"height":600},
		{"id":"98ef2a03e0af42d3","type":"text","text":"[ml-overview-modeling-process.ipynb](https://gist.github.com/eitellauria/09a64150e6107ee29d577606f7507ea8)\n\n[ML_Bias_Variance_Tradeoff_Demo.ipynb](https://gist.github.com/eitellauria/3665fc5119a655c0da44825bf5c86e40)\n\n[ML_Linear_Regression.ipynb](https://gist.github.com/eitellauria/d078daf59337462b81caf51c9511b20d)\n\n[Linear_Regression_with Gradient Descent.ipynb](https://gist.github.com/eitellauria/b09d3b953c56a84e984802294d5b937b)","x":1800,"y":2020,"width":488,"height":200},
		{"id":"f4623cc00a5b5e21","type":"text","text":"Template code for Linear Regression (Pandas, Scikit Learn)\n```Python \n# ===============================================\n# Linear Regression Template in Google Colab\n# Using Pandas & Scikit-Learn\n# ===============================================\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# ===============================================\n# 1. Load Dataset\n# ===============================================\n# TODO: Replace with your dataset path or URL\n# Example: df = pd.read_csv(\"your_dataset.csv\")\ndf = pd.DataFrame()  # Placeholder\n\n# Inspect dataset\nprint(\"Dataset preview:\")\nprint(df.head())\n\n# ===============================================\n# 2. Preprocessing\n# ===============================================\n# TODO: Select features (X) and target (y)\n# Example:\n# X = df[['feature1', 'feature2']]\n# y = df['target']\n\nX = pd.DataFrame()  # Placeholder\ny = pd.Series()     # Placeholder\n\n# Split data into train/test sets\n# Adjust test_size as needed\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# ===============================================\n# 3. Train Model\n# ===============================================\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# ===============================================\n# 4. Predictions\n# ===============================================\ny_pred = model.predict(X_test)\n\n# ===============================================\n# 5. Performance Metrics\n# ===============================================\n# TODO: Add/modify metrics depending on needs\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"\\nPerformance Metrics:\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"RÂ² Score: {r2:.4f}\")\n\n# ===============================================\n# 6. Model Coefficients\n# ===============================================\nprint(\"\\nModel Coefficients:\")\n# TODO: Replace feature names accordingly\n# Example: print(dict(zip(X.columns, model.coef_)))\nprint(\"Intercept:\", model.intercept_)\nprint(\"Coefficients:\", model.coef_)\n\n```","x":-840,"y":1078,"width":920,"height":1662},
		{"id":"4794a676ebe28898","type":"text","text":"Closed-Form Solution of Gradient Descent in Python\nThe optimal vector of weights:\n$$w = (X^TX)^{-1}X^Ty$$\nwhere w includes any w0 = bias b and X is augmented with a first column of 1s. \n1. ignore biases (add an extra feature & weight instead)\n2. get equations from partial derivative\n3. vectorize\n4. write code\nTwo ways: \n```Python \ndef solve_normal_eq(X, y): # Using np.matmul function\n    '''\n    Solve linear regression exactly. (fully vectorized)\n\n    Given `X` - n x m matrix of inputs\n          `y` - n x 1 target outputs\n    Returns the optimal weights as a m-dimensional vector\n    '''\n    n, m = np.shape(X)\n    XTX = np.matmul(X.T, X)\n    XTX_1=np.linalg.inv(XTX)\n    XTY=np.matmul(X.T, y)\n    return np.matmul(XTX_1,XTY)\n```\n```Python\ndef solve_normal_eq(X, y): # Using @ operator\n    '''\n    Solve linear regression exactly. (fully vectorized)\n\n    Given `X` - n x m matrix of inputs\n          `y` - n x 1 target outputs\n    Returns the optimal weights as a m-dimensional vector\n    NOTE that I use the @ operator instead of the np.matmul function.\n         It yileds a cleaner syntax\n    '''\n    n, m = np.shape(X)\n    XTX = X.T @ X\n    XTX_1=np.linalg.inv(XTX)\n    XTY=X.T @ y\n    return XTX_1 @ XTY\n```\n\nThis is the method where we use the *normal equation*, which gives the **optimal weight vector**, i.e., the regression coefficients that minimize the mean squared error.\n\nVectorized gradient descent function:\n```Python \n# Vectorized gradient function\ndef gradfn(weights, X, y):\n    '''\n    weights: a current \"Guess\" of what our weights should be\n          X: matrix of shape (n,m) of input features\n          y: target y values\n    Return gradient of each weight evaluated at the current value\n    '''\n    n, m = np.shape(X)\n    yhat = X @ weights\n    error = yhat - y\n    return (np.transpose(X) @ error)/float(n)\n    \ndef solve_via_gradient_descent(X, y, print_every=100000,\n                               niter=1500000, eta=0.005):    #Try large values of iterations !\n    '''\n    Given `X` - matrix of shape (N,D) of input features\n          `y` - target y values\n    Solves for linear regression weights.\n    Return weights after `niter` iterations.\n    '''\n    n, m = np.shape(X)\n    # initialize all the weights to random values\n    w = np.random.rand(m)\n    for k in range(niter):\n        dw = gradfn(w, X, y)\n        w = w - eta*dw\n        if k % print_every == 0:\n            print (f'Weight after {k} iteration: {str(w)};  gradient: {str(dw)}')\n    return w\n```","x":-840,"y":3060,"width":920,"height":1920},
		{"id":"c9b8bf5ea08ec04b","type":"text","text":"![[Pasted image 20251002201707.png]]","x":80,"y":3520,"width":696,"height":395},
		{"id":"bd06675ea73c64d3","type":"text","text":"![[Pasted image 20251002201737.png]]","x":80,"y":4220,"width":696,"height":484},
		{"id":"4b8e3428d8166f47","type":"text","text":"Stochastic Gradient Descent (Batch Learning):\n```Python\ndef solve_via_SGD(X, y, print_every=25000, batch_size = 50,\n                  niter=1000000, eta=0.01):\n    '''\n    Given `X` - matrix of shape (N,D) of input features\n          `y` - target y values\n          batch_size: size of each minibatch\n    Solves for linear regression weights.\n    Return weights after `niter` iterations.\n    Learning rate `eta`\n    '''\n    n, m = np.shape(X)\n    # initialize all the weights to random values\n    w = np.random.rand(m)\n\n    for k in range(niter):\n        # Randomly sample a minibatch\n        sample = np.random.choice(n, batch_size, replace=False)\n        X_batch = X[sample]\n        y_batch = y[sample]\n\n        dw = gradfn(w, X_batch, y_batch)\n        w = w - eta*dw\n        if k % print_every == 0:\n            loss = np.mean((X_batch @ w - y_batch) ** 2)\n            print(f\"Iteration {k}, Loss={loss:.4f}, Weights={w[:3]}...\")\n    return w\n```","x":-1660,"y":4251,"width":776,"height":729},
		{"id":"15e17b29c8bc971b","type":"text","text":"1) Given a certain problem, choose some *ARCHITECTURE*.\n\tThis may be a decision tree, a linear model, a neural network, a group of trees working together, an ANN/CNN algorithm, etc. \n2) *TIED TO THE TASK*, *DEFINE THE OPTIMIZATION MODEL*.\n\tOur optimization model means the following:\n\t- Find some mechanism to measure the loss function. \n\t- The loss function is tied to the task, not the same across problems.\n\t- $$L = \\frac{1}{2}(\\hat{y} - y)^2$$\n\t- For m points -> J: $$ \\frac{1}{2n} \\sum^{m}_{j = 1}(\\hat{y}^i - y^i)^2$$\n3) *CHOOSE SOLUTION* (can be numerical or analytical)\n4) *VECTORIZE* \n\tMakes things far more elegant, and in terms of the packages we use, they vectorize everything. \n5) *EMIGRATE*","x":223,"y":580,"width":796,"height":498},
		{"id":"a07babc923906112","x":-1985,"y":3740,"width":1101,"height":422,"type":"file","file":"Assets/Pasted image 20251005171609.png"},
		{"id":"2479cc343d44a54e","x":-840,"y":2960,"width":532,"height":84,"type":"text","text":"[Copy of ml_03_assignment_solution.ipynb - Colab](https://colab.research.google.com/drive/1l4mix2HP_7nROfQnGwfzsmXyfq37cu_w?usp=sharing#scrollTo=KAtwBnQ-K1Hn)"}
	],
	"edges":[]
}