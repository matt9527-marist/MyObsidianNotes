{
	"nodes":[
		{"id":"8c7a55b1b1b73fc6","type":"text","text":"**Regularization**","x":-200,"y":-560,"width":160,"height":50},
		{"id":"3a5923de08d38b10","type":"text","text":"**Generalization** - the ability for our model to generalize or make predictions on unknown data. That is, we do not want the model to *overfit* the data or effectively memorize it, nor do we want to have the model be so simple that it *underfits*, or simply does not recognize any patterns.\n\n![[Pasted image 20251003173125.png]]\n\n","x":-944,"y":-480,"width":529,"height":320},
		{"id":"796af5e80dbab677","type":"text","text":"When we select our polynomial model, the only parameter we can control is the model order *m*, which will increase as we add more elements to the vector **w**. The order *m* regulates model complexity. Models that are more complex tend to overfit. ","x":-938,"y":-151,"width":523,"height":131},
		{"id":"d0da8b64b1e06e6d","type":"text","text":"**How do we stop overfitting?**\n*Test holdout*: tests the model's ability to predict new data that was not used  \nin estimating the model, in order to flag problems like overfitting and to give  \nan insight on how the model will generalize to an independent dataset (i.e.,  \nan unknown dataset).  \n*Cross-validation*: same thing but when we don‚Äôt have enough data. Also,  \nand perhaps more important in ML, to tune parameters and therefore  \nchoose the best possible model (more on this later)  \n\tWe will train the model with all of the training data, but leave out the first fold `k=1`. Compute RMSE/MAE. In the next training iteration, we leave out fold `k=2`. Compute RMSE/MAE. Repeating over `k` times, we end up with RMSE and MAE `k`.\n*More data*: As you have more and more data, it becomes more and more  \ndifficult to \"memorize\" the data and its noise. Often, more data translates  \nto the ability to use a more complex model and avoid overfitting.  \n\t‚Ä¢ But there is a limit to how much this helps. If you have a very complex  \nmodel, you need a huge training data set.  \n**Regularization**: add a penalty term to the error function to discourage  \nlearning a more complex model by forcing the model parameters to be  \nsmall.","x":-415,"y":-480,"width":595,"height":520},
		{"id":"fdd37bfbd72f30b0","type":"text","text":"**What is Regularization?**\n- A normal linear regression just tries to find the best line that fits the data by minimizing the error between predictions and actual values.\n    \n- But sometimes the model can overfit‚Äîmeaning it fits the training data too perfectly and does poorly on new data.\n    \n- Regularization fixes this by *shrinking the coefficients* (the weights of features) so that the model doesn‚Äôt depend too much on any single feature.\n\nRecall our model assigns coefficients to the weights: $$\\hat{y} = w_{1}x_{1} + w_{2}x_{2} + \\dots + w_{n}x_{n}$$\nEach weight tells us how much each column or feature affects the predictions made by the model. We do not want our model to be heavily swayed by just one or two of the features, so we shrink the weights. ","x":180,"y":-480,"width":590,"height":460},
		{"id":"b6adf79c73139f85","type":"text","text":"**Ridge Regression** - Add a term *lambda* that controls the relative importance of the data-dependent error J(W) and the regularization term Jw(W)\n![[Pasted image 20251003175408.png]]\nThis particular choice of regularizer is known in the machine learning literature as weight decay or ridge regularizer because, it encourages weight values to decay towards zero, unless supported by the data.\nOur augmented cost function: $$J = \\frac{1}{2m}((Xw-y)^T(Xw-y)) + \\frac{\\lambda}{m}||w||^2_{2}$$\nThis adds the *L2* penalty term, which is the sum of squared weights. Higher lambda values = more penalty. \n- The squared penalty $$\\lambda w^{2}_{j}$$‚Äã makes large weights expensive.\n- To minimize the cost, the algorithm spreads importance more evenly across features instead of letting one dominate.\nRidge regression changes the cost function by punishing large coefficients. This prevents the model from letting one feature‚Äôs weight grow too large, instead forcing it to rely on multiple features in a more balanced way.","x":-1080,"y":60,"width":600,"height":700},
		{"id":"19141c0cead52c04","type":"text","text":"**LASSO** - ‚ÄúLeast Absolute Shrinkage and Selection Operator‚Äù\n![[Pasted image 20251003175924.png]]\nWe compute the sum of the absolute values of the weight component. This is the *lasso* regularizer. \n\nIt has the property that if ùúÜ is sufficiently large, some of the  \ncoefficients ùë§ùëó are driven to zero, leading to a sparse model in which the  \ncorresponding basis loss functions play no role, effectively dropping those features. \n\nThis can solve a few issues suffered by Ridge Regression in terms of how it deals with outliers. Lasso is *less affected by outliers*, since Ridge smoothly spreads influence across all features, and Lasso picks only subsets. \n\nOne problem: \nThe absolute value of **w** is non-differentiable at 0. \n- Instead of solving directly, algorithms use concepts from subgradients (a generalization of gradients for functions with kinks).\n- This is actually _what allows weights to stick at zero_, which is what makes LASSO great for feature selection.\n\n","x":-420,"y":130,"width":600,"height":560},
		{"id":"e2f15e2f0fc1e2fb","type":"text","text":"**Elastic Net** - Combines both Ridge (*L2*) and LASSO (*L1*) regression\n![[Pasted image 20251003180503.png]]\n\nThis type of regularizer is called elastic net as the hyperparameter  \nrho controls how much weight is given to each of the L1 and L2  \npenalties. As such you can have 50/50, 70/30, 80/20 and so on.","x":220,"y":200,"width":606,"height":300},
		{"id":"262627af4a611353","type":"text","text":"**Hyperparameter Tuning**","x":1280,"y":-557,"width":250,"height":60},
		{"id":"660231075b333779","type":"text","text":"Recall: The basic recipe for applying a supervised machine learning model:  \n1. Choose a class of model  \n2. Choose model hyperparameters  \n3. Fit the model to the training data  \n4. Use the model to predict labels for new data","x":1280,"y":-467,"width":424,"height":230},
		{"id":"2d1d25126ee9f6fe","type":"text","text":"‚Ä¢ Now that we've seen the basics of validation and cross-validation, we will go  \ninto a litte more depth regarding model selection and selection of  \nhyperparameters. These issues are some of the most important aspects of  \nthe practice of machine learning, and this information is often glossed over  \nin introductory machine learning courses.  \n‚Ä¢ Of core importance is the following question: if our estimator is  \nunderperforming, how should we move forward? There are several possible  \nanswers:  \n‚Ä¢ Use a more complicated/more flexible model  \n‚Ä¢ Use a less complicated/less flexible model  \n‚Ä¢ Gather more training samples  \n‚Ä¢ Gather more data to add features to each sample  \n‚Ä¢ The answer to this question is often counter-intuitive. In particular,  \nsometimes using a more complicated model will give worse results, and  \nadding more training samples may not improve your results! The ability to  \ndetermine what steps will improve your model is what separates the  \nsuccessful machine learning practitioners from the unsuccessful.","x":1280,"y":-214,"width":467,"height":714},
		{"id":"c2070bb74d84fa6f","type":"text","text":"One way we can go about tuning hyperparameters is by using a **GridSearch**. How do we do this?","x":1780,"y":-467,"width":298,"height":103},
		{"id":"2624f8e2f076f72f","type":"text","text":"Consider when we use **Elastic Net** - the parameters lambda and rho determine how much of ridge vs. how much of lasso we want to use. How do we select these values?\n\nWe shouldn't be testing these hyperparameters against the final testing data. Instead, we are using *validation data* to test them. Doing so gives us an average metric (let's say MAE) on which to determine lambda and rho. ","x":1775,"y":-340,"width":303,"height":340},
		{"id":"4395cd8eefb535ce","type":"text","text":"```Python\nimport numpy as np\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_boston  # example dataset\n\n# 1. Load data\nX, y = load_boston(return_X_y=True)   # or your dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Define the model\nelastic_net = ElasticNet(max_iter=10000)\n\n# 3. Define parameter grid\nparam_grid = {\n    'alpha': np.logspace(-3, 2, 6),      # [0.001, 0.01, 0.1, 1, 10, 100]\n    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # mixture between L1 and L2\n}\n\n# 4. Setup GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=elastic_net,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',  # choose metric\n    cv=5,                              # 5-fold cross-validation\n    n_jobs=-1                          # parallelize\n)\n\n# 5. Fit model\ngrid_search.fit(X_train, y_train)\n\n# 6. Best parameters and score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best CV Score (MSE):\", -grid_search.best_score_)\n\n# 7. Evaluate on test set\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\nprint(\"Test MSE:\", mean_squared_error(y_test, y_pred))\n```","x":2120,"y":-557,"width":900,"height":877},
		{"id":"58b98db26dd301d9","type":"text","text":"**Ridge Regression**\n```Python \n# Fit a pipeline using Training dataset and related labels\n# Use Ridge algorithm for training the model\n#\npipeline = make_pipeline(StandardScaler(), Ridge(alpha=100000000))\npipeline.fit(X_train, y_train)\nprint(f'Intercept (b) {pipeline.steps[1][1].intercept_:.6f}')\nprint(pd.Series(pipeline.steps[1][1].coef_, index=X.columns),'\\n')\n#\n# Calculate the predicted value for training and test dataset\n#\ny_train_pred = pipeline.predict(X_train)\ny_test_pred = pipeline.predict(X_test)\n#\n# Mean Squared Error\nr2_score_train = r2_score(y_train, y_train_pred)\nMSE_train = mean_squared_error(y_train, y_train_pred)\nMSE_test = mean_squared_error(y_test, y_test_pred)\nMAE_train = mean_absolute_error(y_train, y_train_pred)\nMAE_test = mean_absolute_error(y_test, y_test_pred)\nprint(f'R2 train: {r2_score_train:.3f}')\nprint(f'MSE train: {MSE_train:.3f},MSE test:{MSE_test:.3f}')\nprint(f'MAE train: {MAE_train:.3f},MAE test:{MAE_test:.3f}')\n```","x":-1080,"y":800,"width":600,"height":640},
		{"id":"bdb066db52bd9949","type":"text","text":"**Lasso Regression**\n```Python \n# Fit a pipeline using Training dataset and related labels\n# Use the Lasso algorithm for training the model\n#\npipeline = make_pipeline(StandardScaler(), Lasso(alpha =0.001, max_iter=10000))\npipeline.fit(X_train, y_train)\n\nprint(f'Intercept (b) {pipeline.steps[1][1].intercept_:.6f}')\nprint(pd.Series(pipeline.steps[1][1].coef_, index=X.columns),'\\n')\n#\n# Calculate the predicted value for training and test dataset\n#\ny_train_pred = pipeline.predict(X_train)\ny_test_pred = pipeline.predict(X_test)\n#\n# Mean Squared Error\nr2_score_train = r2_score(y_train, y_train_pred)\nMSE_train = mean_squared_error(y_train, y_train_pred)\nMSE_test = mean_squared_error(y_test, y_test_pred)\nMAE_train = mean_absolute_error(y_train, y_train_pred)\nMAE_test = mean_absolute_error(y_test, y_test_pred)\nprint(f'R2 train: {r2_score_train:.3f}')\nprint(f'MSE train: {MSE_train:.3f},MSE test:{MSE_test:.3f}')\nprint(f'MAE train: {MAE_train:.3f},MAE test:{MAE_test:.3f}')\n```","x":-471,"y":800,"width":703,"height":668},
		{"id":"615facff41344329","type":"text","text":"**Elastic Net Regression**\n```Python\n# Fit a pipeline using Training dataset and related labels\n# Use ElasticNet algorithm for training the model\n#\npipeline = make_pipeline(StandardScaler(), ElasticNet(alpha=0.01, l1_ratio=0.3))\npipeline.fit(X_train, y_train)\nprint(f'Intercept (b) {pipeline.steps[1][1].intercept_:.6f}')\nprint(pd.Series(pipeline.steps[1][1].coef_, index=X.columns),'\\n')\n#\n# Calculate the predicted value for training and test dataset\n#\ny_train_pred = pipeline.predict(X_train)\ny_test_pred = pipeline.predict(X_test)\n#\n# Mean Squared Error\nr2_score_train = r2_score(y_train, y_train_pred)\nMSE_train = mean_squared_error(y_train, y_train_pred)\nMSE_test = mean_squared_error(y_test, y_test_pred)\nMAE_train = mean_absolute_error(y_train, y_train_pred)\nMAE_test = mean_absolute_error(y_test, y_test_pred)\nprint(f'R2 train: {r2_score_train:.3f}')\nprint(f'MSE train: {MSE_train:.3f},MSE test:{MSE_test:.3f}')\nprint(f'MAE train: {MAE_train:.3f},MAE test:{MAE_test:.3f}')\n```","x":240,"y":800,"width":855,"height":666},
		{"id":"910258cc69e73897","type":"text","text":"[ML_data_preprocessing.ipynb](https://gist.github.com/eitellauria/8f7e053bbe00cd1aaf6f5db6748e97b5)\n[ml_pipelines.ipynb](https://gist.github.com/eitellauria/b3ad40e871ba13643de353f20cf4a582)\n[ml_regularization.ipynb](https://gist.github.com/eitellauria/7903546bd2fc74fbfb692ffbf13b764a)\n[ml_model_validation_and_hyperparameter_tuning.ipynb](https://gist.github.com/eitellauria/877ee7fdee3a0bfb2dc9b9a30a44aec6)\n","x":220,"y":555,"width":620,"height":135}
	],
	"edges":[]
}