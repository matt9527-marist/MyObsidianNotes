{
	"nodes":[
		{"id":"d8819d887a31ff2f","type":"text","text":"**Data Preprocessing**","x":-440,"y":-500,"width":186,"height":67},
		{"id":"e150c96a8881389f","type":"text","text":"**Transformers**: Some estimators (such as an imputer) can also transform a dataset; these are  \ncalled transformers. Once again, the API is simple: the transformation is performed by  \nthe transform() method with the dataset to transform as a parameter. It returns the transformed  \ndataset. This transformation generally relies on the learned parameters, as is the case for an imputer.  \nAll transformers also have a convenience method called fit_transform() that is equivalent to  \ncalling fit() and then transform() (but sometimes fit_transform() is optimized and runs much faster).","x":-557,"y":-400,"width":420,"height":340},
		{"id":"e362bbffdc596a0e","type":"text","text":"**Estimators**: Any object that can estimate some parameters based on a dataset is called an estimator  \n(e.g., an imputer is an estimator). The estimation itself is performed by the fit() method, and it takes  \nonly a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains  \nthe labels). Any other parameter needed to guide the estimation process is considered a  \nhyperparameter (such as an imputer’s strategy), and it must be set as an instance variable (generally  \nvia a constructor parameter).","x":-993,"y":-400,"width":436,"height":303},
		{"id":"6824f63864ea8938","type":"text","text":"**Predictors**: Finally, some estimators, given a dataset, are capable of making predictions; they are  \ncalled predictors. For example, the LinearRegression model is a predictor. A predictor has  \na predict() method that takes a dataset of new instances and returns a dataset of corresponding  \npredictions. It also has a score() method that measures the quality of the predictions, given a test set  \n(and the corresponding labels, in the case of supervised learning algorithms). See figure below for  \ndetails of the predictor.","x":-137,"y":-400,"width":436,"height":303},
		{"id":"32be24bbebd1902b","type":"file","file":"Assets/Pasted image 20251003023725.png","x":-547,"y":-60,"width":399,"height":269},
		{"id":"9622b79ff54666d4","type":"file","file":"Assets/Pasted image 20251003023739.png","x":-137,"y":-97,"width":338,"height":400},
		{"id":"7b38ed1b8a3116f3","type":"text","text":"**What to do about Missing Data**\nConsider a dataset that has NULL values or missing cells. We have some options:\n- Eliminate the row containing the missing data:\n\t- Can work for few occurrences of missing data\n\t- Can bias the data by obscuring the reason why there is missing data\n- Place a threshold on certain columns/features that decide whether or not to eliminate the feature altogether if the percentage of missing data passes that threshold. \n- *Imputation* - take those columns where we having a NULL value and replace it with some value. How we do this depends on the data. This can be done using the other values in the dataset as the training set to train a model, and then use the model to predict the target missing value.\n\t- Numerical data: use average or median value. \n\t- Discrete / alphanumeric data: use the mode.","x":-993,"y":-91,"width":446,"height":531},
		{"id":"dfc00dc1c21d2e71","type":"text","text":"**Data Encoding**\n(Particularly with alphanumeric data)\nThe algorithms we work with have no use with strings of data. \nRecall:$$\\hat{y} = w_{0} + w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} \\dots$$\nThe coefficients are saying that a unitary increase in the value of say, x1, will introduce a change in y_hat which is equivalent to this value w1 if all of the other features remain constant. This is for scaled values. The distance between 1 and 2, or 2 and 3 is completely meaningless for alphanumeric values (Japan vs. New York for a location feature).\nIn Pandas, we can use dummy variables to solve this problem. In Scikit Learn, this is referred to as **One-Hot Encoding**.","x":-991,"y":440,"width":443,"height":460},
		{"id":"8d173a16826295e8","type":"text","text":"One Hot Encoding Example\n![[Pasted image 20251003024240.png]]","x":-548,"y":209,"width":401,"height":151},
		{"id":"c3900dfd8906f48c","type":"text","text":"• Anyone who has taken a course in statistics that covers linear\nregression has heard some version of the rule regarding pre-\nprocessing categorical predictors with more than two categories and\nthe need to factor them into binary dummy/indicator variables:\n• \"If a variable has k levels, you can create only k-1 indicators. You\nhave to choose one of the k categories as a \"baseline\" and leave out\nits indicator.\"\n• Technically, one can easily create k dummy variables for k categories\nin any software. The reason for not including all k dummies as\npredictors in a linear regression is to avoid perfect multicollinearity,\nwhere an exact linear relationship exists between the k predictors.\nPerfect multicollinearity causes computational and interpretation\nchallenges. This k-dummies issue is also called the Dummy Variable\nTrap.\nWhile these guidelines are required for linear regression, which other  \npredictive models require them? The k-1 dummy rule applies to models where  \nall the predictors are considered together, as a linear combination. Therefore,  \nin addition to linear regression models, the rule would apply to logistic  \nregression models, discriminant analysis, and in some cases to neural networks.","x":-547,"y":360,"width":607,"height":500},
		{"id":"bdea57fdbff04b87","type":"text","text":"**Scaling Numeric Data**\nTwo ways:\nNormalized Scaling (MinMaxScaler): $$X_{Scaled} = \\frac{X - X_{Min}}{X_{Max} - X_{Min}}$$\nIn this case, the scaling places the values between 0 and 1. \n\nStandardized Scaling (Z-scores): $$Z = \\frac{X - \\bar{X}}{\\sigma_{x}}$$\nValues are centered around the mean and scaled as standard deviations.\n\n","x":201,"y":-90,"width":339,"height":470},
		{"id":"b31c56e4803383df","type":"text","text":"Note:\n*Apply fit_transform() on the training data.*\n\n*Apply transform() on the test data.*","x":60,"y":380,"width":452,"height":120},
		{"id":"df6287efed8abe23","type":"text","text":"[ML_data_preprocessing.ipynb](https://gist.github.com/eitellauria/8f7e053bbe00cd1aaf6f5db6748e97b5)","x":60,"y":500,"width":296,"height":56},
		{"id":"731ee1e9997abd17","type":"text","text":"Missing Values Preprocessing\n```Python\n# remove rows that contain missing values\n\ndf.dropna(axis=0)\n# remove columns that contain missing values\n\ndf.dropna(axis=1)\n\n# impute missing values via the column mean using sklearn's SimpleImputer\n\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimr = SimpleImputer(missing_values=np.nan, strategy='mean')\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data\n\n# impute missing values via the column mean using pandas\ndf.fillna(df.mean())\n```","x":-991,"y":940,"width":651,"height":620},
		{"id":"dafae421a7f457ed","type":"text","text":"How to handle categorical data Preprocessing\n\n```Python\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label encoding with sklearn's LabelEncoder\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df['classlabel'].values)\ny\n# array([1, 0, 1])\n\n# reverse mapping\nclass_le.inverse_transform(y)\n\nX = df[['color', 'size', 'price']].values\nprint(\"X:\\n\",X,\"\\n\")\ncolor_le = LabelEncoder()\nX[:, 0] = color_le.fit_transform(X[:, 0])\nprint(\"X using LabelEncoder on first column:\\n\",X,\"\\n\")\n\n# The preferred method though is to use \n# OneHotEncoder to produce dunmy # # variables (1/0 \n# binary integer values) out of the categorical values\nfrom sklearn.preprocessing import OneHotEncoder\n\nX = df[['color', 'size', 'price']].values\nprint(\"X:\\n\",X,\"\\n\")\ncolor_ohe = OneHotEncoder()\nxfirst=color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()\nprint(\"X's first column after using OneHotEncoder:\\n\",xfirst,\"\\n\"\n```","x":-340,"y":940,"width":720,"height":760},
		{"id":"f6771a29db8e2daf","type":"text","text":"Scaling values to the same scale\n\n```Python \nex = np.array([0, 1, 2, 3, 4, 5])\n\nprint('standardized:', (ex - ex.mean()) / ex.std())\n\n# Please note that pandas uses ddof=1 (sample standard deviation)\n# by default, whereas NumPy's std method and the StandardScaler\n# uses ddof=0 (population standard deviation)\n\n# normalize\nprint('normalized:', (ex - ex.min()) / (ex.max() - ex.min()))\n\nstandardized: [-1.46385011 -0.87831007 -0.29277002  0.29277002  0.87831007  1.46385011]\nnormalized: [0.  0.2 0.4 0.6 0.8 1. ]\n```","x":-1091,"y":1560,"width":751,"height":420},
		{"id":"8c0e2437a801fc11","type":"text","text":"**Pipelines**","x":880,"y":-491,"width":119,"height":50},
		{"id":"c147f9d12276b91a","type":"text","text":"Template: Preprocessing a dataset using Scikit-Learn Pipelines\n\nThis template shows how to:\n1. Load a dataset (example: Titanic dataset, or any tabular dataset).\n2. Separate features into numerical and categorical types.\n3. Apply preprocessing pipelines to each type:\n    - Numeric: imputation + scaling\n    - Categorical: imputation + encoding\n4. Combine transformations using `make_column_transformer`.\n5. Create a full pipeline including preprocessing + model training.\n-------------------------\n- SimpleImputer: Handles missing values (strategy='mean', 'median', or 'most_frequent').\n- StandardScaler: Normalizes numerical features (mean=0, variance=1).\n- OneHotEncoder: Encodes categorical variables into 0/1 dummy variables.\n- Pipeline: Chains together preprocessing steps into a sequence.\n- make_column_transformer: Allows applying different pipelines to different columns\n  (numeric vs categorical).\n- train_test_split: Splits data into training and testing sets.","x":880,"y":-400,"width":870,"height":621},
		{"id":"9b4fe041233d72df","type":"text","text":"```Python\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\n# -----------------------------\n# 1. Load Dataset (Example)\n# -----------------------------\n# Replace this with your dataset\ndata = pd.DataFrame({\n    'age': [22, 35, 58, np.nan, 44],\n    'fare': [7.25, 71.83, 8.05, 8.05, np.nan],\n    'sex': ['male', 'female', 'female', 'female', 'male'],\n    'embarked': ['S', 'C', 'Q', np.nan, 'S'],\n    'survived': [0, 1, 1, 1, 0]\n})\n\nX = data.drop('survived', axis=1)\ny = data['survived']\n```","x":880,"y":232,"width":870,"height":648},
		{"id":"72c6cd20af35f638","type":"text","text":"The first step is to define each transformer type. The convention here is generally to create transformers for the different variable types, and then put them together in a column transformer. We can use a small pipeline to impute and the transform the data.\n\n`make_column_transformer` lets you map specific transformations to specific feature subsets, ie. numerical vs. categorical data. \n\n```Python\n# -----------------------------\n# 2. Identify Column Types\n# -----------------------------\nnumeric_features = ['age', 'fare']\ncategorical_features = ['sex', 'embarked']\n\n# -----------------------------\n# 3. Build Preprocessing Pipelines\n# -----------------------------\n\n# Numeric pipeline: Fill missing values (median) + scale\nnumeric_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),    # replace NaN with median\n    ('scaler', StandardScaler())                      # normalize to mean=0, std=1\n])\n\n# Categorical pipeline: Fill missing values + one-hot encode\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # replace NaN with mode\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))    # one-hot encode categorical vars\n])\n# -----------------------------\n# 4. Combine with make_column_transformer\n# -----------------------------\n\"\"\"\nmake_column_transformer allows us to apply different transformations to different sets of columns.\n- First argument: (pipeline, columns)\n- remainder='drop' means all other columns will be dropped unless specified.\n\"\"\"\npreprocessor = make_column_transformer(\n    (numeric_pipeline, numeric_features),\n    (categorical_pipeline, categorical_features),\n    remainder='drop'\n)\n\n```","x":880,"y":880,"width":870,"height":1060},
		{"id":"44316e84b44198be","type":"text","text":"Combine them all into a preprocessor and then make a `model_pipeline` containing the preprocessor and the model we want to use, in this example LogisticRegression(), and we can now input any data we would like through the whole pipeline. In this case, we make a prediction on new data. \n```Python\n# -----------------------------\n# 5. Build Final Pipeline (Preprocessing + Model)\n# -----------------------------\nmodel_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression())\n])\n\n# -----------------------------\n# 6. Train/Test Split and Fit\n# -----------------------------\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel_pipeline.fit(X_train, y_train)\n\n# -----------------------------\n# 7. Evaluate Model\n# -----------------------------\nprint(\"Train Accuracy:\", model_pipeline.score(X_train, y_train))\nprint(\"Test Accuracy:\", model_pipeline.score(X_test, y_test))\n\n# -----------------------------\n# 8. Making Predictions\n# -----------------------------\nsample_input = pd.DataFrame({\n    'age': [30],\n    'fare': [50],\n    'sex': ['female'],\n    'embarked': ['C']\n})\n\nprint(\"Prediction:\", model_pipeline.predict(sample_input))\n```","x":877,"y":1940,"width":877,"height":920},
		{"id":"fbd8ed5f665bfc5d","x":877,"y":2900,"width":510,"height":77,"type":"text","text":"[Copy of ml_assignment_4_solution.ipynb - Colab](https://colab.research.google.com/drive/1o0TYTlIwnGwszw3xTfvdj7Xtzh8HXvwp?usp=sharing)"}
	],
	"edges":[]
}