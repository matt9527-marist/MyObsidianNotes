{
	"nodes":[
		{"id":"f016a161da50810b","type":"text","text":"**Classification** - Classification is a supervised learning method: The  \ntarget variable is specified (data is “labeled”)\n\n- **Linear Regression**:\n    \n    - Predicts a *continuous numerical value*.\n        \n    - Example: Predicting house prices, stock returns, or a person’s weight given features.\n        \n    - Output: Real numbers on the entire number line (ℝ).\n        \n- **Classification**:\n    \n    - Predicts a *categorical label*.\n        \n    - Example: Spam vs. Not Spam (binary classification), or Handwritten digit (0–9).\n        \n    - Output: A discrete class label (finite set of categories).","x":-1228,"y":-143,"width":588,"height":363},
		{"id":"9f8e99323c4a1d2f","type":"text","text":"**Binary Classification** and **Performance Measures**\n**Confusion Matrix**  \n• A very useful device.  \n• Summarizes all the performance metrics data  \ncoming out of a classification task  \n• Counts matches (and mismatches) between actual  \nand predicted class values for each instance of the  \ntarget variable in the training and test data set(s)\n\n**Misclassification Errors**\n• Error = classifying a record as belonging to one  \nclass when it belongs to another class.  \n• Error rate = percent of misclassified records out of  \nthe total records in the validation data  \n• Accuracy (%) = 100 – Error rate","x":-580,"y":-143,"width":620,"height":403},
		{"id":"bc68ff96779ad790","type":"text","text":"Prediction 0","x":-1108,"y":560,"width":162,"height":60},
		{"id":"5e05d4a6a4ca46be","type":"text","text":"Prediction 1","x":-934,"y":560,"width":162,"height":60},
		{"id":"e4c43ea511f56a60","type":"text","text":"Actual 0","x":-1280,"y":640,"width":144,"height":59},
		{"id":"362cab5e1bc433a2","type":"text","text":"Actual 1","x":-1278,"y":720,"width":141,"height":59},
		{"id":"5047f9e39260ea63","type":"text","text":"TN","x":-1108,"y":640,"width":162,"height":59,"color":"1"},
		{"id":"e8ae744dfede4fff","type":"text","text":"FP","x":-934,"y":640,"width":162,"height":59,"color":"5"},
		{"id":"787102c11511f462","type":"text","text":"FN","x":-1108,"y":720,"width":162,"height":59,"color":"5"},
		{"id":"a1c02ce775e3c368","type":"text","text":"TP","x":-934,"y":720,"width":162,"height":59,"color":"1"},
		{"id":"ed984f267c8eee82","type":"text","text":"$$Recall = \\frac{TP}{TP+FN}$$","x":-678,"y":720,"width":278,"height":80},
		{"id":"b4fc4f1bbada3b1c","type":"text","text":"$$FPR = \\frac{FP}{FP+TN}$$","x":-678,"y":619,"width":278,"height":80},
		{"id":"9eb67f9e10998c46","type":"text","text":"$$Precision = \\frac{TP}{TP+FP}$$","x":-956,"y":840,"width":206,"height":80},
		{"id":"cb4179dd1856283a","type":"text","text":"$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$","x":-1348,"y":840,"width":321,"height":80},
		{"id":"0736e064aae47e4d","type":"text","text":"Specifcity = 1 - FPR","x":-678,"y":840,"width":250,"height":60},
		{"id":"fe93a46daf9e08da","type":"text","text":"Recall is sometimes called Specificity or TPR","x":-678,"y":920,"width":250,"height":60},
		{"id":"87415e7e71ad067e","type":"text","text":"**Performance Metrics**\n","x":-1286,"y":360,"width":156,"height":69},
		{"id":"abf4e31b3e124f54","type":"text","text":"**Classification Confusion Matrix:**","x":-1286,"y":460,"width":282,"height":50},
		{"id":"e2295100c650cb12","type":"text","text":"**Area under the ROC Curve (AUC)**","x":-1348,"y":980,"width":309,"height":59},
		{"id":"f195e1e308ed99ad","type":"file","file":"Assets/Pasted image 20251003185937.png","x":-1345,"y":1060,"width":475,"height":360},
		{"id":"8a6e9d30ae4c3988","type":"text","text":"• A ROC curve is the most commonly used way to visualize the performance of  \na binary classifier, and AUC is (arguably) the best way to summarize its  \nperformance in a single number.  \n• ROC (Receiver Operating Characteristic) Curves where developed in WWII to  \nstatistically model false positive and false negative detections of enemy  \nobjects by radar operators","x":-853,"y":1060,"width":489,"height":300},
		{"id":"68ab424358671251","type":"text","text":"*This graphs the TPR (Recall) vs. FPR (1-specificity)*","x":-1345,"y":1440,"width":250,"height":60},
		{"id":"f5b5a6254f3d7224","type":"text","text":"*Goal: Maximize TPR and minimize FPR*  \no Max TPR: classify everything  \npositive  \no Min FPR: classify everything  \nnegative  \no Neither is acceptable","x":-1345,"y":1520,"width":361,"height":184},
		{"id":"6b7477729b9126d6","x":-853,"y":1391,"width":278,"height":59,"type":"text","text":"[ml_sklearn_classif_example.ipynb](https://gist.github.com/eitellauria/8e15334249b231d98c7610588eda6af4)"},
		{"id":"acd83f5baa42411a","x":-1345,"y":1740,"width":604,"height":500,"type":"text","text":"Example Metrics Output: \n```Python\n# Report the predctive performance metrics\n# evaluate predictions\naccuracy = accuracy_score(y_test, y_pred_new)\nprecision=precision_score(y_test, y_pred_new)\nrecall=recall_score(y_test, y_pred_new)\nspecificity=recall_score(y_test, y_pred_new, pos_label=0)\nroc = roc_auc_score(y_test, probs)\n\nprint(\"\\n\")\nprint(f\"Accuracy.........: {accuracy * 100.0:.4f}\")\nprint(f\"Precision........: {precision *100:.4f}\")\nprint(f\"Recall...........: {recall * 100:.4f}\")\nprint(f\"FP Rate...........:{(1-specificity) * 100:.4f}\")\nprint(f\"ROC AUC (probs)..: {roc * 100:.4f}\")\n\ncm = confusion_matrix(y_test, y_pred_new)\nprint(f\"Confusion matrix.:\\n {cm}\")\n\n#This is nicer\nsns.heatmap(cm, annot=True,fmt='d',cbar=False);\n```"},
		{"id":"a28107d30a69c403","x":-340,"y":1060,"width":638,"height":410,"type":"file","file":"Assets/Pasted image 20251003190501.png"},
		{"id":"fca3939ca36d0b94","x":-956,"y":1470,"width":561,"height":234,"type":"text","text":"Depending on the thresholds we assign (risk), a perfect classifier is one that traces the left and top line as shown. We may also use the space underneath to tweak other models, allowing for a little bit of false positive predictions, but keeping a high true positive rate. \n\t- The total area under the top curve is 1.\n\t- The area underneath the main line is 0.5, which indicates a poor classifier, essentially 50/50 random chance to classify correctly. \n\tThe area under the curve of the ROC curve is referred to as **AUC**"},
		{"id":"59d5108885d0a277","type":"text","text":"This does have one problem!\n\t\tIf the data is highly unbalanced, AUC tends to have inflated values. Keep in mind that FPR carries the true negatives that may be unbalanced in the data. ","x":-340,"y":1500,"width":432,"height":191},
		{"id":"f3b992007a143c3c","x":560,"y":-143,"width":150,"height":57,"type":"text","text":"**Probability**"},
		{"id":"2ed6a171a9a011b2","x":560,"y":-86,"width":280,"height":106,"type":"text","text":"The numerical measure of the likelihood that an event will occur, always between 0 and 1."}
	],
	"edges":[
		{"id":"6acaf6246d231043","fromNode":"e8ae744dfede4fff","fromSide":"right","toNode":"b4fc4f1bbada3b1c","toSide":"left"},
		{"id":"79db44fa6087f454","fromNode":"a1c02ce775e3c368","fromSide":"right","toNode":"ed984f267c8eee82","toSide":"left"},
		{"id":"026ebca1142da2e1","fromNode":"a1c02ce775e3c368","fromSide":"bottom","toNode":"9eb67f9e10998c46","toSide":"top"}
	]
}