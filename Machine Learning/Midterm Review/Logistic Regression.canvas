{
	"nodes":[
		{"id":"b56c2bef06c78aae","x":-300,"y":-84,"width":480,"height":384,"type":"text","text":"Linear regression is the starting point for this. We are doing *classification* here, but just using a different architecture. \n\nRecall:\nLinear regression essentially tries to fit a straight line (or hyperplane) to predict a continuous target. \nwhere: $$y = w_{0} + w_{1}x_{1} + w_{2}x_{2} + \\dots$$\nor the target output predicted as: $$y = w^Tx + b$$\nWe cannot use this for classification because we need it to output probabilities not just the main predicted target. \n"},
		{"id":"e424b911056f5f1e","x":-310,"y":320,"width":490,"height":700,"type":"text","text":"**Generalized Form**\n1. Still use a linear predictor that computes a weighted sum of features.\n2. Add a link function: \n![[Pasted image 20250929174617.png]]\nWe have vector `x = [x1, x2, ..., x_m]`\nAnd a scalar `h = summation(x_i + w_i + b)`\n`= w^T * x + b`. *h* computes the dot product of inputs and the weights plus the bias. \n\nThe function `g` is the link function that allows us to *map the linear predictor into a space appropriate for the target variable.* In this case, we want it for classification, so we want the target variable to be output between 0 and 1."},
		{"id":"0b3438168b563c62","x":200,"y":-84,"width":686,"height":804,"type":"text","text":"Logistic regression is the simplest case of a neural network, where there are no hidden layers, and the inputs go directly to the outputs. \n![[Pasted image 20251004174545.png]]\n\nOne example of a linking function `g` that we can use is the **Sigmoid Activation Function**. The sigmoid function takes a real value and maps it to the range [0,1]. Because it is  \nnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier values toward 0 or 1.\n\nThink about it in terms of *neurons*. It is either firing or not firing. We could not use the old linear model for classification because th output is unbounded. The activation function allows us to create lower and upper bounds of 0 and 1. This is convenient because we could equate this output value as a *probability*. \n\n$$\\hat{y} = P(y=1|x)$$ $$1-\\hat{y}=P(y=0|x)$$\n$$P(y=1|x) = g(h) = \\frac{1}{1+e^{-h}} = \\frac{1}{1+e^{\\sum^{m}_{i=1}w_{i}x_{i}+b}}$$\nWhat we have then is the following:\n$$h = w^Tx + b$$ $$\\hat{y} = \\frac{1}{1+e^{-h}} = sigmoid(h)$$"},
		{"id":"3e5e10dfce62bf04","x":920,"y":-84,"width":800,"height":884,"type":"text","text":"**Learning Logistic Regression**\n• How are the parameters of the model, (the weights and bias) learned?  \n\t✓ We set up an architecture  \n\t✓ We define a cost function  \n\t✓ We compute the gradient of the cost function  \n\t✓ We either derive a closed solution, if it exists, or use gradient descent\n\nWe have the problem of *nonlinearity*. The function we are using is not convex (when graphed, it is not a nice bowl shaped line). We are in a situation where we may find ourselves in a **local minimum**, which means that we need to change the kind of cost function that we are using:\n![[Pasted image 20251004174824.png]]\n\nThis function is called the **Cross-Entropy** loss function. \n$$\\hat{y} = P(y=1|x)$$ $$1-\\hat{y}=P(y=0|x)$$\nThere is a more elegant way of writing the above expressions:\n$$P(y|x) = \\hat{y}^{y} * (1-\\hat{y})^{1-y}$$\nTherefore, if `y=1`, then we will just get `yhat`, and if `y=0`, then we just get `(1-yhat)`\nIf we take logarithms, we obtain the loss function:\n$$L(y,\\hat{y})=-y\\log(\\hat{y})-(1-y)\\log(1-\\hat{y})$$\nThis is convex, so it works much better. "},
		{"id":"d1976548faa3f49f","x":715,"y":-200,"width":342,"height":66,"type":"text","text":"**Connectionism and Logistic Regression**"},
		{"id":"e738ecb07c73990a","x":1760,"y":-84,"width":1051,"height":1544,"type":"text","text":"$$h=w^Tx + b$$$$\\hat{y} = \\frac{1}{1+e^{-h}} = sigmoid(h)$$\nIf `P(y = 1 | x) = yhat` then `P(y = 0 | x) = 1 - yhat`\nWe more elegantly wrote this as: $$P(y|x) = \\hat{y}^y (1-\\hat{y})^{(1-y)}$$\nOverall, we will have:\n$$\\begin{bmatrix}\nh\n\\end{bmatrix} = \n\\begin{bmatrix}\nX\n\\end{bmatrix} *\n\\begin{bmatrix}\nW\n\\end{bmatrix}\n+\\begin{bmatrix}\nb\n\\end{bmatrix}*1$$\n`h: 1xm, X: mxm, w: mx1, b:mx1`\n$$\\vec{h} = X^T * \\vec{w} + b \\vec{1}$$\n$$\\begin{bmatrix}\n\\hat{y}\n\\end{bmatrix} = \\begin{bmatrix}\nsigmoid(h)\n\\end{bmatrix}$$\nEach of the elements of `yhat` is computed as the sigmoid of h2, h2, h3 over the observations. \n$$\\hat{y} = sigmoid(\\vec{h})$$\nNow we need to generalize this loss function for all of the data. \nIn a similar manner as in our Bayesian learning data, each data point is contributing some probability. \n$$P (data | x) = P(y_{1}|x) * P(y_{2}|x) * \\dots *P(y_{n}|x)$$This product is called a **likelihood**, gathering all of the probabilities from all the data points. \n$$= \\prod^{m}_{i=1}\\hat{y}_{i}^{y_{i}}(1-\\hat{y}_{i})^{1-y_{i}}$$\n- Take logs \n- Change sign \n- Average over *m*\n\nBy taking logs, we are transforming the product expression into a sum of the terms:\n$$J(w, b) = -\\frac{1}{m}\\sum^{m}_{i=1} y^{(i)}\\log \\hat{y}_{(i)} + (1-y^{i})\\log(1-\\hat{y}_{{(i)}})$$\nThe above first term of the expression is a *dot product*.\n$$J(w,b) = -\\frac{1}{m} \\begin{bmatrix}\ny\\log \\hat{y}+(1-y)^T\\log(1-\\hat{y})\n\\end{bmatrix}$$\nWe cannot find an analytical solution to this due to the nonlinearity. We will use gradient descent like with vanilla linear regression:\n$$\\begin{bmatrix}\nw \\leftarrow w - \\eta/ \\frac{\\partial J}{\\partial W}\n\\end{bmatrix},\n\\begin{bmatrix}\nb \\leftarrow w - \\eta/ \\frac{\\partial J}{\\partial W}\n\\end{bmatrix}$$\nWe need to compute the partial derivative of our cost function J w.r.t. the weights W.\n$$\\frac{ \\partial J }{ \\partial W } = \\frac{ \\partial J }{ \\partial \\hat{y} } * \\frac{ \\partial \\hat{y} }{ \\partial h } * \\frac{ \\partial h }{ \\partial w } $$\nGoing one by one:\n$$\\frac{ \\partial J }{ \\partial \\hat{y} } = -\\frac{1}{m}\\begin{bmatrix}\n\\frac{y}{\\hat{y}}-\\frac{(1-y)}{1-\\hat{y}}\n\\end{bmatrix}$$\n$$\\frac{ \\partial \\hat{y} }{ \\partial h } = \\hat{y} \\otimes (1-\\hat{y})$$\n$$\\frac{ \\partial h }{ \\partial w } = X$$\nComputing the product of all of this:\n$$= \\frac{1}{m}X^T(\\hat{y}-y)$$This is our gradient for linear models, exactly the same formulation. This is the average of the dot product of the data times the error. Doing this for *b* gives us simply: $$\\frac{ \\partial J }{ \\partial b } = \\frac{1}{m}(\\hat{y}-y) $$This gives us all that we need to do computations. "}
	],
	"edges":[]
}