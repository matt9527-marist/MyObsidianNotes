{
	"nodes":[
		{"id":"d1976548faa3f49f","x":-300,"y":-180,"width":342,"height":66,"type":"text","text":"**Connectionism and Logistic Regression**"},
		{"id":"b56c2bef06c78aae","x":-300,"y":-84,"width":480,"height":384,"type":"text","text":"Linear regression is the starting point for this. We are doing *classification* here, but just using a different architecture. \n\nRecall:\nLinear regression essentially tries to fit a straight line (or hyperplane) to predict a continuous target. \nwhere: $$y = w_{0} + w_{1}x_{1} + w_{2}x_{2} + \\dots$$\nor the target output predicted as: $$y = w^Tx + b$$\nWe cannot use this for classification because we need it to output probabilities not just the main predicted target. \n"},
		{"id":"e424b911056f5f1e","x":-310,"y":320,"width":490,"height":700,"type":"text","text":"**Generalized Form**\n1. Still use a linear predictor that computes a weighted sum of features.\n2. Add a link function: \n![[Pasted image 20250929174617.png]]\nWe have vector `x = [x1, x2, ..., x_m]`\nAnd a scalar `h = summation(x_i + w_i + b)`\n`= w^T * x + b`. *h* computes the dot product of inputs and the weights plus the bias. \n\nThe function `g` is the link function that allows us to *map the linear predictor into a space appropriate for the target variable.* In this case, we want it for classification, so we want the target variable to be output between 0 and 1."},
		{"id":"0b3438168b563c62","x":200,"y":-84,"width":686,"height":388,"type":"text","text":"Logistic regression is the simplest case of a neural network, where there are no hidden layers, and the inputs go directly to the outputs. \n![[Pasted image 20251004174545.png]]\n\nOne example of a linking function `g` that we can use is the **Sigmoid Activation Function**. The sigmoid function takes a real value and maps it to the range [0,1]. Because it is  \nnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier values toward 0 or 1.\n\n"}
	],
	"edges":[]
}