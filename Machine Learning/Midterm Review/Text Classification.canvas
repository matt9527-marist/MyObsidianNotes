{
	"nodes":[
		{"id":"c0478c44d8727dc0","type":"text","text":"**Event Models**\n*Categorical Model* - for when the data is discrete, smoothed by adding a 1 in the numerator and the cardinality in the denominator to avoid dealing with a zero frequency.\n*BernoulliNB* - For when the data is one-hot encoded. \n*Continuous (GaussianNB)* - For when the data is normally distributed. The probability becomes a probability density. The mean and the standard deviation change for each of the attributes. \n\n**Multinomial** - A distribution of counts.\nImagine the following situation: we have a jar of red, green, and blue marbles, 20 in all. We want to compute the probability of extracting from the jar exactly 2 R, 2 G, and 2 B. Multinomial probabilities predict over counts. \n   Suppose: 8R, 3G, 9B\n   The multinomial distribution: $$P(N_{R},N_{G},N_{B}) = \\frac{N!}{N_{R}!,N_{G}!,N_{B}!}P_{R}^{N_{R}}*P_{G}^{N_{G}}*P_{B}^{N_{B}}$$\n   Notice the factorial operations. This is because we can get many configurations for a given result. We are in a combinatorial situation. \n   In this case, the probabilities are: P_R = 8/20, P_G = 3 / 20, P_B = 9 / 20\n\n   And we obtain: $$P(N_{1}\\dots N_{k}) = \\frac{N!}{\\prod^{k}_{j=1}}\\prod^{k}_{j=1}P_{j}^{N_{j}}$$","x":-345,"y":-177,"width":645,"height":657},
		{"id":"89127b61a941e8fe","type":"text","text":"**NB Review**\nWithin Bayesian Learning, we have a joint probability of the data and the target:\n`P(x,y) = P(x|y) * P(y)`\nThis is a way of finding the criterion for classification. The value that maximizes this joint probability. We are taking the joint probability of the data and the target, and whichever class maximizes it is the choice of the model. \nIn the framework which we have been working, we need to find a probabilistic architecture by defining a *loss function* that we need to minimize. \n- We use logarithms because it is a monotonic function. We get negative numbers instead of increasingly small multiplied numbers. \n- `logP(x,y) = logP(x|y) * logP(y)`\n$$J = -\\log P(x,y) = -\\log P(x|y)-\\log P(y)$$\nComing up with the solution to this loss function requires a lot of data. To make this easier, we can use **Naive Bayes**, and assume that all of the inputs are independent. \n$$P(\\vec{x}|y) = \\prod^{m}_{j=1}P(x_{j}|y)$$\nWith this we obtain:\n$$J = -\\sum^{m}_{j=1}\\log P(x_{j}|y)-\\log P(y)$$","x":-960,"y":-177,"width":580,"height":697},
		{"id":"f26a19ded516c294","x":-960,"y":600,"width":198,"height":58,"type":"text","text":"**Text Classification**"},
		{"id":"3667ebb36ff84345","x":-960,"y":680,"width":511,"height":188,"type":"text","text":"Text classification and recognition is a very common and widely  \napplicable use of machine learning.  \n• Spam filters  \n• Automatic classification of news  \n• Learn to classify web pages by topic  \n• Sentiment Analysis"},
		{"id":"a02278553871a1d0","x":-963,"y":880,"width":514,"height":360,"type":"text","text":"Suppose we have *m*, which is a corpus of documents: j_1, j_2, ..., j_m as X\nLet's say those texts are the complete works of Shakespeare or something that has a signature that can differentiate the documents from each other. \nWe want to infer from the data which document is which (Y).\nWhat problems do we find here?\n- We have been working with data that is for the most part of *fixed length*. \n- Documents like books are not fixed length.\nOne possible approach: \n- Build a **term frequency** matrix:"}
	],
	"edges":[]
}