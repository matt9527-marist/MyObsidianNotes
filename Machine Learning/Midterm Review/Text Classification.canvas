{
	"nodes":[
		{"id":"c0478c44d8727dc0","type":"text","text":"**Event Models**\n*Categorical Model* - for when the data is discrete, smoothed by adding a 1 in the numerator and the cardinality in the denominator to avoid dealing with a zero frequency.\n*BernoulliNB* - For when the data is one-hot encoded. \n*Continuous (GaussianNB)* - For when the data is normally distributed. The probability becomes a probability density. The mean and the standard deviation change for each of the attributes. \n\n**Multinomial** - A distribution of counts.\nImagine the following situation: we have a jar of red, green, and blue marbles, 20 in all. We want to compute the probability of extracting from the jar exactly 2 R, 2 G, and 2 B. Multinomial probabilities predict over counts. \n   Suppose: 8R, 3G, 9B\n   The multinomial distribution: $$P(N_{R},N_{G},N_{B}) = \\frac{N!}{N_{R}!,N_{G}!,N_{B}!}P_{R}^{N_{R}}*P_{G}^{N_{G}}*P_{B}^{N_{B}}$$\n   Notice the factorial operations. This is because we can get many configurations for a given result. We are in a combinatorial situation. \n   In this case, the probabilities are: P_R = 8/20, P_G = 3 / 20, P_B = 9 / 20\n\n   And we obtain: $$P(N_{1}\\dots N_{k}) = \\frac{N!}{\\prod^{k}_{j=1}}\\prod^{k}_{j=1}P_{j}^{N_{j}}$$","x":-345,"y":-177,"width":645,"height":657},
		{"id":"89127b61a941e8fe","type":"text","text":"**NB Review**\nWithin Bayesian Learning, we have a joint probability of the data and the target:\n`P(x,y) = P(x|y) * P(y)`\nThis is a way of finding the criterion for classification. The value that maximizes this joint probability. We are taking the joint probability of the data and the target, and whichever class maximizes it is the choice of the model. \nIn the framework which we have been working, we need to find a probabilistic architecture by defining a *loss function* that we need to minimize. \n- We use logarithms because it is a monotonic function. We get negative numbers instead of increasingly small multiplied numbers. \n- `logP(x,y) = logP(x|y) * logP(y)`\n$$J = -\\log P(x,y) = -\\log P(x|y)-\\log P(y)$$\nComing up with the solution to this loss function requires a lot of data. To make this easier, we can use **Naive Bayes**, and assume that all of the inputs are independent. \n$$P(\\vec{x}|y) = \\prod^{m}_{j=1}P(x_{j}|y)$$\nWith this we obtain:\n$$J = -\\sum^{m}_{j=1}\\log P(x_{j}|y)-\\log P(y)$$","x":-960,"y":-177,"width":580,"height":697},
		{"id":"f26a19ded516c294","type":"text","text":"**Text Classification**","x":-960,"y":600,"width":198,"height":58},
		{"id":"3667ebb36ff84345","type":"text","text":"Text classification and recognition is a very common and widely  \napplicable use of machine learning.  \n• Spam filters  \n• Automatic classification of news  \n• Learn to classify web pages by topic  \n• Sentiment Analysis","x":-960,"y":680,"width":511,"height":188},
		{"id":"a02278553871a1d0","type":"text","text":"Suppose we have *m*, which is a corpus of documents: j_1, j_2, ..., j_m as X\nLet's say those texts are the complete works of Shakespeare or something that has a signature that can differentiate the documents from each other. \nWe want to infer from the data which document is which (Y).\nWhat problems do we find here?\n- We have been working with data that is for the most part of *fixed length*. \n- Documents like books are not fixed length.\nOne possible approach: \n- Build a **term frequency** matrix:","x":-963,"y":880,"width":514,"height":360},
		{"id":"8c9bc9526eebedcb","type":"file","file":"Assets/Pasted image 20251004165124.png","x":-400,"y":680,"width":565,"height":300},
		{"id":"92ad248a5a9b2da5","type":"text","text":"In the following example, we want to determine which documents are spam or ham.","x":-400,"y":980,"width":389,"height":87},
		{"id":"00ca65f21ae3941e","type":"file","file":"Assets/Pasted image 20251004165201.png","x":-400,"y":1067,"width":565,"height":309},
		{"id":"05e0cb34c30051d8","type":"text","text":"Since this is *supervised learning*, we will first fit/train a model on training data. \nAs always, before we fit the model, we split the data into train  \nand test sets  \nOnce we have the model’s predictions from the X_test data, we  \ncompare it to the true y values (the correct labels).","x":-376,"y":1416,"width":536,"height":224},
		{"id":"db5258f2ae017af4","type":"file","file":"Assets/Pasted image 20251004165440.png","x":220,"y":1060,"width":895,"height":580},
		{"id":"a478ffbd13c628b9","type":"text","text":"**Feature Extraction from Text**\nHow do we count the occurence of each word to map the text to a number? \n\nUse a **Bag of Words** (BOW):\n- Intuition: Use the frequency of words in each document to identify the document / categorization.\n- Order does not matter that much when simply classifying text. \n\nHow?\n1) Build a dictionary V of words from the data. \n2) Index the words in V \n3) Each document in a corpus of docs is transformed into a matrix  \nof dimension (N , |V| ) containing the number of occurrences of  \na word w(j) (as indexed in V) in document d\n\nThis matrix is called a *document term matrix*. \n![[Pasted image 20251004165819.png]]","x":-376,"y":1680,"width":796,"height":820},
		{"id":"7451e72e87c6650a","type":"text","text":"Keep in mind the categorical model: `P(x|y) = (Nij + 1) / (Ni + kj)`.\nWhat we want to do is compute the probability: `P(d,yi) = P(d|yi) * P(yi)`\nwhere *d* is a given document that we want to predict the class of. How do we compute `P(d|yi)`?\n\n$$P(d|y_{i}) \\propto \\frac{N!}{\\prod^{m}_{t=1}}\\prod^{m}_{t=1}P(w_{t}|y_{i})^{N_{t}}$$\nWhich is the same as the above multinomial probability model. \nRecall that we have been using logarithms for classification: \n$$-\\log P(d, y_{i}) = -\\log P(d|y_{i}) - \\log P(y_{i})$$\nAnd now we need to calculate the probability of a word given the class:\n$$P(w_{t}|y_{i}) = \\frac{count(w_{t},y_{i}) + 1}{\\sum^{ }_{w \\in |v|}count(w,y_{i}) + |v|}$$\nWhere `count(wt,yi)` is the count of the number of words in a given class over the summation of all the words in the dictionary. This is the full expression essentially the same thing as the categorical model's equation. Recall that this equation is Laplace smoothed according to the size of the dictionary,\n\nThus we can proceed by using logarithms to simplify the multiplication steps into addition.\n$$\\min -\\log P(d,y_{i}) = -\\sum^{m}_{j=1}N^{t}\\log P(w_{t}|y_{i}) -\\log P(y_{i})$$\nThis entire expression above can be placed in terms of vectors simply as:\n$$-\\log P(d,y_{i}) = \\vec{t}_{f}^{T}\\log \\vec{p} + b$$\nThe summation can be written elegantly as a *vector of term frequencies* times the logarithm of each of the individual probabilities of the words for a given class, plus *b*, which is the bias or the priors of the classes. ","x":449,"y":1675,"width":1311,"height":685},
		{"id":"fc820ed6135502fd","type":"text","text":"**Manual Bag of Words Example**\n\n![[Pasted image 20251004170645.png]]\nX = The text data from each of the 10 documents. \nY = |2|, 2 classes, let's say class \"Fox\" and class \"Empires\"\n\nTo create a bag of words, we simply build a vector of counts, where the columns consist of each word from both of the class texts.\n![[Pasted image 20251004170917.png]]\n\nWe estimate the priors of each category C1 and C2. $$\\hat{P}(c_{i}) = \\frac{N_{c_{i}}}{N_{D}}$$ which is just going to be 50/50, since there are 10 documents in total, and 5 of each document belong to either class. So our priors are 0.5 and 0.5. \n![[Pasted image 20251004171102.png]]\nThis is now a table of probabilities, as we would expect from Naive Bayes. Recall the equation is Laplace Smoothed:\n- `count(wt,ci) + 1` is the number of times the word `wt` appears in the document belonging to class `ci` + 1. \n- `|V| + sum(count(w, ci))` is the total number of word occurrences in the class + the cardinality of the word count. \n“How often does this word appear in documents of class `ci`​, compared to the total number of words in that class, adjusted so that no word ever has probability zero?”","x":-376,"y":2640,"width":1256,"height":1220},
		{"id":"7e226f4929e7d7b9","x":951,"y":2640,"width":809,"height":960,"type":"text","text":"![[Pasted image 20251004171632.png]]\nObviously, we can see from the words used, that this new text should be class C2, \"Empires\".\n![[Pasted image 20251004171739.png]]\n![[Pasted image 20251004171837.png]]\n\nNow we can compute:\n![[Pasted image 20251004171856.png]]\nRemember that this is essentially the dot product. "},
		{"id":"c998ee48a6e792e4","x":1800,"y":2640,"width":800,"height":740,"type":"text","text":"Simpler Example:\nDoc1: \"I love machine learning\"\nDoc2: \"I love deep learning\"\nList all unique words across the documents:\n\nV={I,love,machine,learning,deep}\n\nFor each document, count how many times each vocabulary word appears.\n\n- **Doc 1:** \"I love machine learning\"\n    - I → 1\n    - love → 1\n    - machine → 1\n    - learning → 1\n    - deep → 0\n    → Vector = **[1, 1, 1, 1, 0]**\n- **Doc 2:** \"I love deep learning\n    - I → 1\n    - love → 1\n    - machine → 0\n    - learning → 1\n    - deep → 1\n    → Vector = **[1, 1, 0, 1, 1]**\n![[Pasted image 20251004172610.png]]\n"},
		{"id":"f10bea2af3820d7b","x":2660,"y":2640,"width":931,"height":1719,"type":"text","text":"Multinomial Bayes with SkLearn Spam or Ham\n```Python\n# ============================\n# Bag of Words Template (sklearn)\n# ============================\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB  # Example classifier\nfrom sklearn.pipeline import Pipeline\n\n# ----------------------------\n# 1. Load your text data\n# ----------------------------\n# Suppose you have a text file with one document per line\nwith open(\"my_text_data.txt\", \"r\", encoding=\"utf-8\") as f:\n    documents = f.readlines()\n\n# Example labels (for supervised learning)\n# In real use, load labels from a CSV or another file\nlabels = [0, 1, 0, 1, 1]   # 0 = ham, 1 = spam, etc.\n\n# ----------------------------\n# 2. Split into train/test sets\n# ----------------------------\nX_train, X_test, y_train, y_test = train_test_split(documents, labels, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\n# ----------------------------\n# 3. Create a Bag-of-Words Vectorizer\n# ----------------------------\nvectorizer = CountVectorizer(\n    lowercase=True,       # convert all text to lowercase\n    stop_words=\"english\", # remove common stop words\n    max_features=5000     # keep top-N most frequent words\n)\n\n# ----------------------------\n# 4. Build a Pipeline with a Classifier\n# ----------------------------\n# The pipeline ensures that text is vectorized and then classified\nmodel = Pipeline([\n    ('vectorizer', vectorizer),\n    ('classifier', MultinomialNB())\n])\n\n# ----------------------------\n# 5. Train (\"fit\") the Model\n# ----------------------------\nmodel.fit(X_train, y_train)\n\n# ----------------------------\n# 6. Evaluate\n# ----------------------------\naccuracy = model.score(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.3f}\")\n\n# ----------------------------\n# 7. Inspect Vocabulary / Feature Matrix\n# ----------------------------\n# Get BoW features for training data\nX_train_bow = model.named_steps['vectorizer'].transform(X_train)\nprint(\"BoW Matrix Shape:\", X_train_bow.shape)\n\n# Show feature names\nprint(model.named_steps['vectorizer'].get_feature_names_out()[:20])  # first 20 words\n\n```"},
		{"id":"7c2710ebc2b1c2a1","x":1800,"y":3420,"width":660,"height":480,"type":"text","text":"**Using SkLearn to simply convert a text into BOW**\n```Python \n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer() # run it plain vanilla\n\nprint(count_vect)\n\ntf= count_vect.fit_transform(df.text)\nprint(tf.shape)\ntf\n\nprint(tf.toarray())\n\nprint(count_vect.get_feature_names_out())\n\n#['around' 'children' 'fire' 'man' 'sat' 'started' 'the']\n```"},
		{"id":"2067b077fb30e35b","x":1840,"y":1675,"width":882,"height":445,"type":"file","file":"Assets/Pasted image 20251004173138.png"},
		{"id":"240d1108de158c06","x":2842,"y":1675,"width":749,"height":445,"type":"file","file":"Assets/Pasted image 20251004173159.png"},
		{"id":"f7632461f74a80ec","x":1840,"y":2200,"width":1130,"height":320,"type":"file","file":"Assets/Pasted image 20251004173307.png"},
		{"id":"660346472f1bea3f","x":3760,"y":1675,"width":1530,"height":965,"type":"file","file":"Assets/Pasted image 20251004173346.png"},
		{"id":"2256a3747459b62d","type":"text","text":"[ml_bow_exercise.ipynb](https://gist.github.com/eitellauria/a754b5a3045800e8484060cb09237487)\n\n[ml_spamfilter_nb_sklearn.ipynb](https://gist.github.com/eitellauria/efec9b85b7f9f69c25fe29134fef3d3b)\n\n[ml_coding-a_spamfilter_with_multinomialnb.ipynb](https://gist.github.com/eitellauria/da08cea7a55dd6aa5774f6f7e752cc58)","x":452,"y":2400,"width":431,"height":160}
	],
	"edges":[]
}